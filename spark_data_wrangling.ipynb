{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This projects uses Spark to consume 3 different datasets in order to analyize, clean, and then output parquet files for future analysis\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType, DateType\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project will consume and analyze data on NYC Crime stats, population count, and temperature. It will use Spark to analyze the datasets and then write parquet files to an S3 storage for downstream analysis in order the answer the following questions:\n",
    "1) What differences in crime exist between the 5 boroughs?\n",
    "2) What is the crime-per-population count in the 5 boroughs?\n",
    "3) Is there a correlation between rate of crime and the temperature?\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "**NYPD Complaint Data**: Comes from a Kaggle Data set and describes crimes reported the various NYC burrows from 2013 - 2015.\n",
    "\n",
    "**World Temperature Data**: comes from Kaggle and contains average weather temperatures hourly by city in Kelvin from 2013 - 2017.\n",
    "\n",
    "**NYC Borough Population**: Provided by NYC Open Data. Lists the population of each of the 5 boroughs from 1950 through projected 2040. This is calculated every 10 years, so to fit with the rest of the data set timeframe the population count from 2010 will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "input_path = config['S3']['IN_BUCKET']\n",
    "census_df = spark.read.json(input_path + 'New_York_City_Population_by_Borough__1950_-_2040.json')\n",
    "temp_df = spark.read.option('header',True).csv(input_path + 'US_City_Hourly_Temp_2013-2017.csv')\n",
    "crime_df = spark.read.option('header',True).csv(input_path + 'NYPD_Complaint_Data_Historic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Census Data: \n",
    "    1) Drop unnecessary columns\n",
    "    2) format the population count as an integer\n",
    "    3) trim borough names to remove whitespace\n",
    "    \n",
    "Temperature Data:\n",
    "    1) Drop unnecessary columns\n",
    "    2) drop records with null temperature values\n",
    "    3) cast values to correct data types\n",
    "    4) use timestamp value to group and average temperature for each day\n",
    "    5) convert kelvin data to celsius and fahrenheit \n",
    "    6) round values to 2 significant digits\n",
    "   \n",
    "Crime Data:\n",
    "    1) drop unnecessary columns\n",
    "    2) drop records will null values that will impede analysis\n",
    "    3) convert text value fields into usable TimeStamp type\n",
    "    4) cast columns into correct data type(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#CENSUS DATA#\n",
    "trunc_census_df = census_df.select(F.col(\"borough\"),\n",
    "                                 F.col(\"2010\")\n",
    "                                )\n",
    "\n",
    "#type case columns\n",
    "cleaned_census_df = trunc_census_df.withColumn(\"borough\",F.trim(F.col(\"borough\")).cast(StringType())) \\\n",
    "                    .withColumn(\"2010\",F.regexp_replace(F.col(\"2010\"), \",\", \"\").cast(IntegerType()))\n",
    "\n",
    "#stage dataframe for final output\n",
    "staging_census_df = cleaned_census_df.select(F.col(\"borough\"),\n",
    "                                           F.col(\"2010\").alias(\"pop_count\")\n",
    "                                          ).drop_duplicates()\n",
    "\n",
    "print('Total Rows: {}'.format(staging_census_df.count()))\n",
    "staging_census_df.printSchema()\n",
    "staging_census_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TEMPERATURE DATA#\n",
    "trunc_temp_df = temp_df.select(F.col(\"datetime\").cast(TimestampType()),\n",
    "                                 F.col(\"New York\")\n",
    "                                )\n",
    "\n",
    "#drop null temperature values, convert remaining temps to double\n",
    "trunc_temp_df = trunc_temp_df.dropna(how='any', subset=['New York'])\n",
    "trunc_temp_df = trunc_temp_df.withColumn(\"New York\", F.col(\"New York\").cast(DoubleType()))\n",
    "trunc_temp_df = trunc_temp_df.withColumn(\"date\", F.to_date(F.col(\"DateTime\")))\n",
    "\n",
    "# Create udf to convert Kelvin temp to Celsius\n",
    "@F.udf(DoubleType())\n",
    "def kelvin_to_celsius(x):\n",
    "    if x:\n",
    "        return x - 273.15\n",
    "    return None\n",
    "\n",
    "#create udf to convert celsius to fahrenheit\n",
    "@F.udf(DoubleType())\n",
    "def kelvin_to_fahrenheit(x):\n",
    "    if x:\n",
    "        return (x - 273.15) * (9/5) + 32\n",
    "    return None\n",
    "\n",
    "#collapse dates, calculate average temperature\n",
    "avg_temp_df = trunc_temp_df.groupBy(\"date\").agg(F.mean('New York').cast(DoubleType()).alias(\"kelvin\"))\n",
    "\n",
    "#calculate celcius and farenheit\n",
    "staging_temp_df = avg_temp_df.withColumn(\"celsius\", kelvin_to_celsius(F.col(\"kelvin\"))) \\\n",
    "                             .withColumn(\"fahrenheit\", kelvin_to_fahrenheit(F.col(\"kelvin\")))\n",
    "\n",
    "#round temp to 2 decimals\n",
    "staging_temp_df = staging_temp_df.withColumn(\"kelvin\", F.round(F.col(\"kelvin\"),2)) \\\n",
    "                         .withColumn(\"celsius\", F.round(F.col(\"celsius\"),2)) \\\n",
    "                         .withColumn(\"fahrenheit\", F.round(F.col(\"fahrenheit\"),2))\n",
    "\n",
    "staging_temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#CRIME DATA#\n",
    "#limit df to selected columns\n",
    "trunc_crime_df = crime_df.select(F.col(\"CMPLNT_NUM\"),\n",
    "                                 F.col(\"BORO_NM\"),\n",
    "                                 F.col(\"CMPLNT_FR_DT\"),\n",
    "                                 F.col(\"CMPLNT_FR_TM\"),\n",
    "                                 F.col(\"OFNS_DESC\"),\n",
    "                                 F.col(\"LAW_CAT_CD\")\n",
    "                                )\n",
    "\n",
    "#drop null complaint date values -- can't tie to specific date-time\n",
    "#also drop any nulls in crime description or type columns\n",
    "trunc_crime_df = trunc_crime_df.dropna(how='any', subset=['CMPLNT_FR_DT','CMPLNT_FR_TM','OFNS_DESC','LAW_CAT_CD'])\n",
    "\n",
    "#combine and format date and time column into single column to generate date value\n",
    "cleaned_crime_df = trunc_crime_df.withColumn('TimeStamp',F.from_unixtime(F.unix_timestamp( \\\n",
    "                                          F.concat_ws(' ',trunc_crime_df.CMPLNT_FR_DT, trunc_crime_df.CMPLNT_FR_TM), 'MM/dd/yyy HH:mm:ss')))\n",
    "    \n",
    "cleaned_crime_df = cleaned_crime_df.withColumn('CMPLNT_FR_DT', F.to_date(F.col(\"TimeStamp\")))\n",
    "\n",
    "#type case columns\n",
    "cleaned_crime_df = cleaned_crime_df.withColumn(\"CMPLNT_NUM\",F.col(\"CMPLNT_NUM\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"CMPLNT_FR_DT\",F.col(\"CMPLNT_FR_DT\").cast(DateType())) \\\n",
    "                    .withColumn(\"OFNS_DESC\",F.col(\"OFNS_DESC\").cast(StringType())) \\\n",
    "                    .withColumn(\"LAW_CAT_CD\",F.col(\"LAW_CAT_CD\").cast(StringType())) \\\n",
    "                    .withColumn(\"BORO_NM\",F.initcap(F.col(\"BORO_NM\")).cast(StringType()))\n",
    "\n",
    "#stage dataframe for final output\n",
    "staging_crime_df = cleaned_crime_df.select(F.col(\"CMPLNT_NUM\").alias(\"id\"),\n",
    "                                           F.col(\"BORO_NM\").alias(\"borough\"), \n",
    "                                           F.col(\"CMPLNT_FR_DT\").alias(\"date\"),\n",
    "                                           F.col(\"OFNS_DESC\").alias(\"crime_desc\"),\n",
    "                                           F.col(\"LAW_CAT_CD\").alias(\"crime_cat\")\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Dimension tables:\n",
    "\n",
    "    crime_categories_table:\n",
    "        crime_index\n",
    "        crime_cat\n",
    "        crime_desc\n",
    "\n",
    "    boroughs_table:\n",
    "        borough_index\n",
    "        borough\n",
    "        pop_count\n",
    "\n",
    "    temperature_table:\n",
    "        temp_index\n",
    "        temp_date\n",
    "        kelvin\n",
    "        celsius\n",
    "        fahrenheit\n",
    "\n",
    "    date_table:\n",
    "        date_index\n",
    "        date\n",
    "        dayofweek\n",
    "        month\n",
    "        year\n",
    "        \n",
    "Fact Table:\n",
    "\n",
    "    crime_stats_df:\n",
    "    id\n",
    "    date_index\n",
    "    borough_index\n",
    "    temp_index\n",
    "    crime_index\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1) using above created dataframes join them together to populate the subsequent dimension and fact tables\n",
    "2) output to parquet files for later analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Temperature dimension table\n",
    "temp_dim_df = staging_temp_df.join(date_dim_df, staging_temp_df.date == date_dim_df.date)\\\n",
    "              .select(date_dim_df.date.alias('temp_date'),\n",
    "                      F.col(\"kelvin\"),\n",
    "                      F.col(\"celsius\"),\n",
    "                      F.col(\"fahrenheit\")\n",
    "                     ).sort(\"temp_date\").drop_duplicates()\n",
    "\n",
    "temp_dim_df = temp_dim_df.withColumn('temp_index', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
    "\n",
    "temp_dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#borough dimension table\n",
    "borough_dim_df = staging_crime_df.join(staging_census_df, staging_crime_df.borough == staging_census_df.borough)\\\n",
    "                .select(staging_crime_df.borough,staging_census_df.pop_count).sort(F.col(\"borough\")).drop_duplicates()\n",
    "\n",
    "borough_dim_df = borough_dim_df.withColumn('borough_index', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
    "borough_dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Crime category dimension table\n",
    "crime_dim_df = staging_crime_df.select(F.col(\"crime_desc\"),F.col(\"crime_cat\")).drop_duplicates()\n",
    "\n",
    "crime_dim_df = crime_dim_df.withColumn('crime_index', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
    "\n",
    "crime_dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Final Fact table for cime stats\n",
    "fact_dim_df = staging_crime_df.join(date_dim_df, staging_crime_df.date == date_dim_df.date)\\\n",
    "                              .join(crime_dim_df,'crime_desc')\\\n",
    "                              .join(borough_dim_df,'borough')\\\n",
    "                              .join(temp_dim_df,staging_crime_df.date == temp_dim_df.temp_date)\\\n",
    "                              .select(staging_crime_df.id, date_dim_df.date_index, borough_dim_df.borough_index, temp_dim_df.temp_index, crime_dim_df.crime_index)\\\n",
    "                              .sort(date_dim_df.date)\\\n",
    "                              .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def table_exists(df):\n",
    "    if df is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "if table_exists(fact_dim_df) & table_exists(crime_dim_df) & table_exists(borough_dim_df) & table_exists(date_dim_df) & table_exists(temp_dim_df):\n",
    "    print(\"data quality check passed\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"data quality check failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def table_not_empty(df):\n",
    "    return df.count() != 0 \n",
    "\n",
    "if table_not_empty(fact_dim_df) & table_not_empty(crime_dim_df) & table_not_empty(borough_dim_df) & table_not_empty(date_dim_df) & table_not_empty(temp_dim_df):\n",
    "    print(\"data quality check passed!\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"data quality check failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "The data included in this project was designed as a one-time snap shot of crime statistics and as such need not necessarily be updated again. However, it can be updated as frequently as supported by fresh data sets and from business requirements on analysis output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Explanation Spark is chosen for this project as it is known for processing large amount of data fast (with in-memory compute), scales easily with additional worker nodes, has the ability to digest different data formats (e.g. Parquet, CSV, JSON), and integrates nicely with cloud storage like S3 and warehouse like Redshift.\n",
    "\n",
    "There are also considerations in terms of scaling existing solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "If the data was increased by 100x:\n",
    "\n",
    "We could spin up larger instances of EC2s hosting Spark and/or additional Spark work nodes. With added capacity arising from either vertical scaling or horizontal scaling, we should be able to accelerate processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "If the data populates a dashboard that must be updated on a daily basis by 7am every day:\n",
    "\n",
    "In this instance the pipeline could be shifted to Airflow to schedule and automate the data pipeline jobs. Built-in retry and monitoring mechanism can enable us to meet user requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "If the database needed to be accessed by 100+ people:\n",
    "\n",
    "We could host the solution in a cloud data warehouse, with larger capacity to serve more users, and workload management to ensure equitable usage of resources across users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
